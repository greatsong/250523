"""
AI ì„¤ë¬¸ ëŒ€ì‹œë³´ë“œÂ ğŸš€
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… ë¹ ë¥´ê²Œ ì‹œì‘í•˜ë ¤ë©´(Implementation Tips) ì„¹ì…˜ ë°˜ì˜ ë²„ì „
   1. ë‹¨ë‹µ/ì¥ë¬¸ ìë™ íŒë³„(50ì ê¸°ì¤€)Â â†’Â configs ê°±ì‹ 
   2. ì´ˆê²½ëŸ‰ í•œê¸€ í† í°í™” + ë¶ˆìš©ì–´ ì œê±° â†’â€¯ë¹ˆë„ ë¶„ì„
   3. ëŒ€ê·œëª¨ ì¥ë¬¸ ìš”ì•½: 2,000ì ì²­í¬â€‘ìš”ì•½ â†’ ì¬ìš”ì•½(Recursive)
   4. PlotlyÂ theme í†µì¼(plotly_white)
   5. ê° ë¶„ì„ ì„¹ì…˜ì— "ğŸ’¡í•´ì„¤"Â Markdown ì•ˆë‚´ ì¶”ê°€
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import koreanize_matplotlib  # ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­
import re, json, textwrap, io, base64, os, random
from datetime import datetime
from collections import Counter
from typing import Dict, List
from openai import OpenAI
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Plotly ê¸°ë³¸ í…Œë§ˆ í†µì¼ (Tipâ€¯4)
px.defaults.template = "plotly_white"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Web UI ê¸°ë³¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config("AI ì„¤ë¬¸ ëŒ€ì‹œë³´ë“œ", "ğŸ¤–", layout="wide")
ST_CSS = """
<style>
.main-header{font-size:2.5rem;font-weight:700;text-align:center;
background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);
-webkit-background-clip:text;-webkit-text-fill-color:transparent;margin:1rem 0;}
.section-header{font-size:1.6rem;font-weight:600;border-bottom:3px solid #667eea;margin:1.3rem 0 .8rem 0;}
.metric-card{background:linear-gradient(135deg,#f5f7fa 0%,#c3cfe2 100%);padding:1rem;border-radius:12px;text-align:center;box-shadow:0 4px 12px rgba(0,0,0,.08);}
</style>"""
st.markdown(ST_CSS, unsafe_allow_html=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
COLUMN_TYPES = {
    "timestamp":"íƒ€ì„ìŠ¤íƒ¬í”„","email":"ì´ë©”ì¼","phone":"ì „í™”ë²ˆí˜¸","name":"ì´ë¦„",
    "student_id":"í•™ë²ˆ/ì‚¬ë²ˆ","numeric":"ìˆ«ì","single_choice":"ë‹¨ì¼ ì„ íƒ",
    "multiple_choice":"ë‹¤ì¤‘ ì„ íƒ","linear_scale":"ì²™ë„","text_short":"ì§§ì€ í…ìŠ¤íŠ¸",
    "text_long":"ê¸´ í…ìŠ¤íŠ¸","url":"URL","other":"ê¸°íƒ€"
}
STOP_KO = {
    'ì€','ëŠ”','ì´','ê°€','ì„','ë¥¼','ì˜','ì—','ì™€','ê³¼','ë„','ë¡œ','ìœ¼ë¡œ','ë§Œ','ì—ì„œ','ê¹Œì§€','ë¶€í„°',
    'ë¼ê³ ','í•˜ê³ ','ìˆë‹¤','ìˆëŠ”','ìˆê³ ','í•©ë‹ˆë‹¤','ì…ë‹ˆë‹¤','ëœë‹¤','í•˜ë©°','í•˜ì—¬','í–ˆë‹¤','í•œë‹¤'
}
TOKEN_REGEX = re.compile(r"[ê°€-í£]{2,}")  # Tipâ€¯2Â : ì´ˆê²½ëŸ‰ í•œê¸€ í† í°í™”

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
for k,v in [("df",None),("configs",{}),("ai",{}),("ai_done",False)]:
    if k not in st.session_state: st.session_state[k]=v

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ AI Analyzer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class AIAnalyzer:
    def __init__(self,key:str):
        self.key=key
        self.client=OpenAI(api_key=key) if key else None
        self.model="gpt-4o"

    # ì»¬ëŸ¼ íƒ€ì… ì¶”ë¡  (ê·œì¹™ + GPT)
    def infer_types(self,df:pd.DataFrame)->Dict[str,str]:
        heur={}
        for c in df.columns:
            s=str(df[c].dropna().iloc[0]) if not df[c].dropna().empty else ""
            if re.fullmatch(r"\d{4}[./-]",s[:5]): heur[c]="timestamp"
            elif "@" in s: heur[c]="email"
            elif s.isdigit() and len(s)<=6: heur[c]="student_id"
            elif s.startswith("http"): heur[c]="url"
            else: heur[c]="other"
        if not self.client:
            return heur
        prompt=textwrap.dedent(f"""
        í—¤ë”+ìƒ˜í”Œ:
        {df.head(3).to_csv(index=False)}
        ê¸°ì¡´ ì¶”ì •: {json.dumps(heur,ensure_ascii=False)}
        ê°œì„ í•˜ì—¬ JSON ë°˜í™˜.
        íƒ€ì…: {', '.join(COLUMN_TYPES)}
        """)
        try:
            res=self.client.chat.completions.create(
                model=self.model,
                messages=[{"role":"user","content":prompt}],
                temperature=0
            ).choices[0].message.content
            res=re.sub(r"^```json|```$","",res,flags=re.I).strip()
            g=json.loads(res)
            return {c:(g.get(c,heur[c]) if g.get(c) in COLUMN_TYPES else heur[c]) for c in df.columns}
        except:
            return heur

    # GPT í…ìŠ¤íŠ¸ ìš”ì•½ (ë‹¨ì¼ í˜¸ì¶œìš©)
    def summarize(self,texts:List[str],q:str)->str:
        if not self.client or len(texts)==0:
            return "-"
        prompt=textwrap.dedent(f"""
        Q: {q}
        í•œêµ­ì–´ ì‘ë‹µì„ ì½ê³  í•µì‹¬ ì¸ì‚¬ì´íŠ¸ 3ì¤„ ìš”ì•½ bullet ë°˜í™˜:
        {json.dumps(texts,ensure_ascii=False)}
        """)
        try:
            return self.client.chat.completions.create(
                model=self.model,
                messages=[{"role":"user","content":prompt}],
                temperature=0.3
            ).choices[0].message.content.strip()
        except:
            return "ìš”ì•½ ì‹¤íŒ¨"

    # ëŒ€ìš©ëŸ‰(>2000ì) ì¬ê·€ ìš”ì•½ (Tipâ€¯3)
    def summarize_large(self,texts:List[str],q:str)->str:
        if len(texts)==0:
            return "-"
        # 1) í•„ìš”ì‹œ ìƒ˜í”Œë§(ì‘ë‹µ 1000ê°œ ì´ˆê³¼)  ---------------------
        if len(texts)>1000:
            texts=random.sample(texts,1000)
        # 2) 2,000ì ì²­í¬ ë‹¨ìœ„ë¡œ 1ì°¨ ìš”ì•½ ------------------------
        chunks,buf=[],[]
        char_sum=0
        for t in texts:
            buf.append(t)
            char_sum+=len(t)
            if char_sum>2000:
                chunks.append(self.summarize(buf,q))
                buf,char_sum=[],0
        if buf:
            chunks.append(self.summarize(buf,q))
        # 3) 2ì°¨(ìµœì¢…) ìš”ì•½ --------------------------------------
        if len(chunks)==1:
            return chunks[0]
        else:
            return self.summarize(chunks,q)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Utils â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def simple_tokenize(text:str)->List[str]:
    """ì´ˆê²½ëŸ‰ í•œê¸€ í† í°í™” (Tipâ€¯2)"""
    return TOKEN_REGEX.findall(text)

def freq_top(tokens:List[str],n:int=20):
    return Counter([t for t in tokens if t not in STOP_KO]).most_common(n)

def find_korean_font():
    for p in [
        "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
        "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc",
        "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf"
    ]:
        if os.path.exists(p):
            return p
    return None

def wordcloud_base64(text:str)->str:
    font=find_korean_font()
    wc=WordCloud(font_path=font,background_color="white",width=800,height=400).generate(text)
    fig,ax=plt.subplots(figsize=(8,4))
    ax.imshow(wc, interpolation="bilinear")
    ax.axis("off")
    buf=io.BytesIO()
    plt.tight_layout(pad=0)
    fig.savefig(buf,format="png",bbox_inches="tight")
    plt.close(fig)
    return "data:image/png;base64,"+base64.b64encode(buf.getvalue()).decode()

def ts_info(series:pd.Series):
    ts=pd.to_datetime(series,errors="coerce").dropna()
    if ts.empty: return None
    return {"hour":ts.dt.hour.value_counts().sort_index(),
            "day":ts.dt.date.value_counts().sort_index(),
            "heat":ts.dt.to_period('H').value_counts()}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Sidebar â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with st.sidebar:
    api_key=st.text_input("OpenAI API Key",value=st.secrets.get("openai_api_key",""),type="password")
    mask_opt=st.checkbox("ğŸ”’ ê°œì¸ì •ë³´ ë§ˆìŠ¤í‚¹",True)
    auto_type=st.checkbox("ğŸ¤– ì»¬ëŸ¼ ìë™ ì¶”ë¡ ",True)

file=st.file_uploader("CSV ì—…ë¡œë“œ",type="csv")
if not file:
    st.stop()

df=pd.read_csv(file)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì»¬ëŸ¼ íƒ€ì… ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ai=AIAnalyzer(api_key) if api_key else None
if auto_type and api_key and not st.session_state.configs:
    st.session_state.configs=ai.infer_types(df)
if not st.session_state.configs:
    st.session_state.configs={c:"other" for c in df.columns}
configs=st.session_state.configs

# â–¶ Tipâ€¯1: ë‹¨ë‹µ/ì¥ë¬¸ ìë™ íŒë³„(50ì ê¸°ì¤€) -----------------------
for col in df.columns:
    if configs[col] in ["other","text_short","text_long"]:
        max_len=df[col].astype(str).str.len().dropna().max()
        if pd.isna(max_len):
            continue
        configs[col]="text_short" if max_len<50 else "text_long"

# ì»¬ëŸ¼ íƒ€ì… ìˆ˜ë™ ìˆ˜ì • -------------------------------------------
with st.expander("ì»¬ëŸ¼ íƒ€ì… í™•ì¸/ìˆ˜ì •", False):
    c1,c2=st.columns(2)
    for i,col in enumerate(df.columns):
        with (c1 if i%2==0 else c2):
            configs[col]=st.selectbox(col,list(COLUMN_TYPES.keys()),
                index=list(COLUMN_TYPES.keys()).index(configs[col]),
                format_func=lambda x:COLUMN_TYPES[x],key=f"type_{col}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Navigation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
page=st.radio("ë©”ë‰´",["ğŸ“Š ê°œìš”","ğŸ“ˆ í†µê³„","ğŸ“ í…ìŠ¤íŠ¸ ë¶„ì„"],horizontal=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. ê°œìš” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if page=="ğŸ“Š ê°œìš”":
    st.markdown('<h2 class="section-header">ğŸ“Š ì „ì²´ ê°œìš”</h2>',unsafe_allow_html=True)
    tot,ques=len(df),len(df.columns)
    comp=df.notna().sum().sum()/(tot*ques)*100
    st.metric("ì‘ë‹µ",tot)
    st.metric("ì§ˆë¬¸",ques)
    st.metric("í‰ê·  ì™„ë£Œìœ¨",f"{comp:.1f}%")
    resp=(df.notna().sum()/tot*100).sort_values()
    st.plotly_chart(px.bar(x=resp.values,y=resp.index,orientation="h",
        labels={'x':'ì‘ë‹µë¥ (%)','y':'ì§ˆë¬¸'},color=resp.values,color_continuous_scale="viridis"),
        use_container_width=True,key="overview")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2. í†µê³„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
elif page=="ğŸ“ˆ í†µê³„":
    st.markdown('<h2 class="section-header">ğŸ“ˆ ì„ íƒí˜•Â·ì‹œê°„ ë¶„ì„</h2>',unsafe_allow_html=True)
    st.markdown("ğŸ’¡ **í•´ì„¤**: ì„ íƒí˜• ê²°ê³¼ì˜ ë¶„í¬ì™€ ì œì¶œ ì‹œê°„ì„ í™•ì¸í•  ìˆ˜ ìˆì–´ìš”.")
    # íƒ€ì„ìŠ¤íƒ¬í”„ heatmap
    ts_cols=[c for c,t in configs.items() if t=="timestamp"]
    if ts_cols:
        ts=ts_info(df[ts_cols[0]])
        if ts:
            st.subheader("â° ë‚ ì§œÃ—ì‹œê°„ Heatmap")
            heat_df=ts['heat'].reset_index()
            heat_df['date']=heat_df['index'].astype(str)
            heat_df[['date','hour']]=heat_df['date'].str.split(' ',expand=True)
            heat_df['hour']=heat_df['hour'].str[:2]
            pivot=heat_df.pivot("date","hour","count").fillna(0)
            st.plotly_chart(px.imshow(pivot,aspect="auto",labels={'x':'ì‹œê°„','y':'ë‚ ì§œ','color':'ì‘ë‹µìˆ˜'}),use_container_width=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. í…ìŠ¤íŠ¸ ë¶„ì„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
else:
    st.markdown('<h2 class="section-header">ğŸ“ í…ìŠ¤íŠ¸ ì‘ë‹µ ë¶„ì„</h2>',unsafe_allow_html=True)
    st.markdown("ğŸ’¡ **í•´ì„¤**: ë¹ˆë„ BarÂ·WordCloud, ì¥ë¬¸ ìš”ì•½ì„ í•œëˆˆì— í™•ì¸í•©ë‹ˆë‹¤.")
    for col,t in configs.items():
        if t not in ["text_short","text_long"]:
            continue
        st.subheader(f"{col} ({'ë‹¨ë‹µ' if t=='text_short' else 'ì¥ë¬¸'})")
        texts=[str(x) for x in df[col].dropna() if str(x).strip()]
        if len(texts)==0:
            st.info("ì‘ë‹µ ì—†ìŒ")
            continue
        # ---------- ë¹ˆë„ ë¶„ì„ ----------
        tokens=[tok for txt in texts for tok in simple_tokenize(txt)]
        freq=freq_top(tokens)
        if freq:
            words,counts=zip(*freq)
            fig=px.bar(x=counts,y=words,orientation="h",labels={'x':'ë¹ˆë„','y':'ë‹¨ì–´'})
            st.plotly_chart(fig,use_container_width=True)
            # WordCloud
            wc_img=wordcloud_base64(' '.join(tokens))
            st.image(wc_img,use_column_width=True)
        # ---------- ì¥ë¬¸ ìš”ì•½ ----------
        if t=="text_long" and api_key:
            with st.spinner("AI ìš”ì•½ ìƒì„± ì¤‘..."):
                summary=ai.summarize_large(texts,col)
            st.success("### ğŸ“ 3â€‘ì¤„ ìš”ì•½\n"+summary)
        st.divider()
