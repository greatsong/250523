"""
AI ì„¤ë¬¸ ëŒ€ì‹œë³´ë“œ ğŸš€
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- ë‹¨ë‹µ/ì¥ë¬¸ ìë™ íŒë³„(50ì ê¸°ì¤€)
- ì´ˆê²½ëŸ‰ í•œê¸€ í† í°í™” + ë¹ˆë„ ë¶„ì„
- GPT ëŒ€ìš©ëŸ‰ ìš”ì•½(2â€¯ë‹¨ê³„)
- ì„ íƒí˜•/ë‹¤ì¤‘ì„ íƒ/ì²™ë„ ì‹œê°í™”
- Plotly ê¸°ë³¸ í…Œë§ˆ(plotly_white)
- í•œê¸€ WordCloud í°íŠ¸ ìë™ ë‹¤ìš´ë¡œë“œ â†’ ê¹¨ì§ ë°©ì§€
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import koreanize_matplotlib                              # ìš”êµ¬ì‚¬í•­
import re, json, textwrap, io, base64, os, random
from datetime import datetime
from collections import Counter
from typing import Dict, List
from openai import OpenAI
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from matplotlib import font_manager                      # í°íŠ¸ ì„¤ì •ìš©
import urllib.request, tempfile, pathlib                 # <â”€ NEW

# â”€ Plotly í…Œë§ˆ í†µì¼
px.defaults.template = "plotly_white"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Web UI ê¸°ë³¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config("AI ì„¤ë¬¸ ëŒ€ì‹œë³´ë“œ", "ğŸ¤–", layout="wide")
ST_CSS = """
<style>
.main-header{font-size:2.5rem;font-weight:700;text-align:center;
background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);
-webkit-background-clip:text;-webkit-text-fill-color:transparent;margin:1rem 0;}
.section-header{font-size:1.6rem;font-weight:600;border-bottom:3px solid #667eea;margin:1.3rem 0 .8rem 0;}
.metric-card{background:linear-gradient(135deg,#f5f7fa 0%,#c3cfe2 100%);padding:1rem;border-radius:12px;text-align:center;box-shadow:0 4px 12px rgba(0,0,0,.08);}
</style>"""
st.markdown(ST_CSS, unsafe_allow_html=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
COLUMN_TYPES = {
    "timestamp": "íƒ€ì„ìŠ¤íƒ¬í”„", "email": "ì´ë©”ì¼", "phone": "ì „í™”ë²ˆí˜¸", "name": "ì´ë¦„",
    "student_id": "í•™ë²ˆ/ì‚¬ë²ˆ", "numeric": "ìˆ«ì",
    "single_choice": "ë‹¨ì¼ ì„ íƒ", "multiple_choice": "ë‹¤ì¤‘ ì„ íƒ",
    "linear_scale": "ì²™ë„",
    "text_short": "ì§§ì€ í…ìŠ¤íŠ¸", "text_long": "ê¸´ í…ìŠ¤íŠ¸",
    "url": "URL", "other": "ê¸°íƒ€"
}
STOP_KO = {
    'ì€','ëŠ”','ì´','ê°€','ì„','ë¥¼','ì˜','ì—','ì™€','ê³¼','ë„','ë¡œ','ìœ¼ë¡œ','ë§Œ','ì—ì„œ','ê¹Œì§€','ë¶€í„°',
    'ë¼ê³ ','í•˜ê³ ','ìˆë‹¤','ìˆëŠ”','ìˆê³ ','í•©ë‹ˆë‹¤','ì…ë‹ˆë‹¤','ëœë‹¤','í•˜ë©°','í•˜ì—¬','í–ˆë‹¤','í•œë‹¤'
}
TOKEN_REGEX = re.compile(r"[ê°€-í£]{2,}")                 # ì´ˆê²½ëŸ‰ í† í°í™”

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
for k, v in [("df", None), ("configs", {}), ("ai", {}), ("ai_done", False)]:
    if k not in st.session_state:
        st.session_state[k] = v

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ AI Analyzer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class AIAnalyzer:
    def __init__(self, key: str):
        self.key = key
        self.client = OpenAI(api_key=key) if key else None
        self.model = "gpt-4o"

    # ì»¬ëŸ¼ íƒ€ì… ì¶”ë¡  (ê·œì¹™ + GPT)
    def infer_types(self, df: pd.DataFrame) -> Dict[str, str]:
        heur = {}
        for c in df.columns:
            s = str(df[c].dropna().iloc[0]) if not df[c].dropna().empty else ""
            if re.fullmatch(r"\d{4}[./-]", s[:5]): heur[c] = "timestamp"
            elif "@" in s: heur[c] = "email"
            elif s.isdigit() and len(s) <= 6: heur[c] = "student_id"
            elif s.startswith("http"): heur[c] = "url"
            else: heur[c] = "other"

        if not self.client:
            return heur

        prompt = textwrap.dedent(f"""
        í—¤ë”+ìƒ˜í”Œ:
        {df.head(3).to_csv(index=False)}
        ê¸°ì¡´ ì¶”ì •: {json.dumps(heur, ensure_ascii=False)}
        ê°œì„ í•˜ì—¬ JSON ë°˜í™˜.
        íƒ€ì…: {', '.join(COLUMN_TYPES)}
        """)
        try:
            res = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0
            ).choices[0].message.content
            res = re.sub(r"^```json|```$", "", res, flags=re.I).strip()
            g = json.loads(res)
            return {c: (g.get(c, heur[c]) if g.get(c) in COLUMN_TYPES else heur[c]) for c in df.columns}
        except Exception:
            return heur

    # GPT í…ìŠ¤íŠ¸ ìš”ì•½
    def summarize(self, texts: List[str], q: str) -> str:
        if not self.client or len(texts) == 0:
            return "-"
        prompt = textwrap.dedent(f"""
        Q: {q}
        í•œêµ­ì–´ ì‘ë‹µì„ ì½ê³  í•µì‹¬ ì¸ì‚¬ì´íŠ¸ 3ì¤„ ìš”ì•½ bullet ë°˜í™˜:
        {json.dumps(texts, ensure_ascii=False)}
        """)
        try:
            return self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3
            ).choices[0].message.content.strip()
        except Exception:
            return "ìš”ì•½ ì‹¤íŒ¨"

    # ëŒ€ìš©ëŸ‰ ì¬ê·€ ìš”ì•½
    def summarize_large(self, texts: List[str], q: str) -> str:
        if len(texts) == 0:
            return "-"
        if len(texts) > 1000:                    # 1) í•„ìš” ì‹œ ìƒ˜í”Œë§
            texts = random.sample(texts, 1000)

        # 2) 2,000ì ì²­í¬
        chunks, buf, char_sum = [], [], 0
        for t in texts:
            buf.append(t)
            char_sum += len(t)
            if char_sum > 2000:
                chunks.append(self.summarize(buf, q))
                buf, char_sum = [], 0
        if buf:
            chunks.append(self.summarize(buf, q))

        # 3) ìµœì¢… ìš”ì•½
        return chunks[0] if len(chunks) == 1 else self.summarize(chunks, q)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Utils â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def find_korean_font() -> str:
    """í•œê¸€ í°íŠ¸ ê²½ë¡œ íƒìƒ‰ âœ ì—†ìœ¼ë©´ NanumGothic ë‹¤ìš´ë¡œë“œ"""
    candidates = [
        "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
        "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc",
        "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf"
    ]
    for p in candidates:
        if os.path.exists(p):
            return p

    # ì—†ìœ¼ë©´ Google Fontsì—ì„œ ë‹¤ìš´ë¡œë“œ
    url = ("https://raw.githubusercontent.com/google/fonts/main/"
           "ofl/nanumgothic/NanumGothic-Regular.ttf")
    tmp_path = pathlib.Path(tempfile.gettempdir()) / "NanumGothic.ttf"
    if not tmp_path.exists():                    # ìµœì´ˆ 1íšŒë§Œ
        urllib.request.urlretrieve(url, tmp_path)
    return str(tmp_path)

# â–¶ matplotlib ê¸°ë³¸ í°íŠ¸ ì„¤ì •
plt.rcParams["font.family"] = font_manager.FontProperties(
    fname=find_korean_font()).get_name()

def simple_tokenize(text: str) -> List[str]:
    return TOKEN_REGEX.findall(text)

def freq_top(tokens: List[str], n: int = 20):
    return Counter([t for t in tokens if t not in STOP_KO]).most_common(n)

def wordcloud_base64(text: str) -> str:
    wc = WordCloud(
        font_path=find_korean_font(),
        background_color="white",
        width=800, height=400
    ).generate(text)
    fig, ax = plt.subplots(figsize=(8, 4))
    ax.imshow(wc, interpolation="bilinear")
    ax.axis("off")
    buf = io.BytesIO()
    plt.tight_layout(pad=0)
    fig.savefig(buf, format="png", bbox_inches="tight")
    plt.close(fig)
    return "data:image/png;base64," + base64.b64encode(buf.getvalue()).decode()

def ts_info(series: pd.Series):
    ts = pd.to_datetime(series, errors="coerce").dropna()
    if ts.empty:
        return None
    return {
        "hour": ts.dt.hour.value_counts().sort_index(),
        "day": ts.dt.date.value_counts().sort_index(),
        "heat": ts.dt.to_period('H').value_counts()
    }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Sidebar â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with st.sidebar:
    api_key = st.text_input(
        "OpenAI API Key",
        value=st.secrets.get("openai_api_key", ""),
        type="password"
    )
    mask_opt = st.checkbox("ğŸ”’ ê°œì¸ì •ë³´ ë§ˆìŠ¤í‚¹", True)
    auto_type = st.checkbox("ğŸ¤– ì»¬ëŸ¼ ìë™ ì¶”ë¡ ", True)

file = st.file_uploader("CSV ì—…ë¡œë“œ", type="csv")
if not file:
    st.stop()

df = pd.read_csv(file)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì»¬ëŸ¼ íƒ€ì… ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ai = AIAnalyzer(api_key) if api_key else None
if auto_type and api_key and not st.session_state.configs:
    st.session_state.configs = ai.infer_types(df)
if not st.session_state.configs:
    st.session_state.configs = {c: "other" for c in df.columns}
configs = st.session_state.configs

# ë‹¨ë‹µ/ì¥ë¬¸ ìë™ íŒë³„(50ì)
for col in df.columns:
    if configs[col] in ["other", "text_short", "text_long"]:
        max_len = df[col].astype(str).str.len().dropna().max()
        if pd.isna(max_len):
            continue
        configs[col] = "text_short" if max_len < 50 else "text_long"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Manual Type Edit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with st.expander("ì»¬ëŸ¼ íƒ€ì… í™•ì¸/ìˆ˜ì •", False):
    c1, c2 = st.columns(2)
    for i, col in enumerate(df.columns):
        with (c1 if i % 2 == 0 else c2):
            configs[col] = st.selectbox(
                col,
                list(COLUMN_TYPES.keys()),
                index=list(COLUMN_TYPES.keys()).index(configs[col]),
                format_func=lambda x: COLUMN_TYPES[x],
                key=f"type_{col}"
            )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Navigation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
page = st.radio("ë©”ë‰´", ["ğŸ“Š ê°œìš”", "ğŸ“ˆ í†µê³„", "ğŸ“ í…ìŠ¤íŠ¸ ë¶„ì„"], horizontal=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. ê°œìš” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if page == "ğŸ“Š ê°œìš”":
    st.markdown('<h2 class="section-header">ğŸ“Š ì „ì²´ ê°œìš”</h2>', unsafe_allow_html=True)
    tot, ques = len(df), len(df.columns)
    comp = df.notna().sum().sum() / (tot * ques) * 100
    st.metric("ì‘ë‹µ", tot)
    st.metric("ì§ˆë¬¸", ques)
    st.metric("í‰ê·  ì™„ë£Œìœ¨", f"{comp:.1f}%")
    resp = (df.notna().sum() / tot * 100).sort_values()
    st.plotly_chart(
        px.bar(
            x=resp.values, y=resp.index, orientation="h",
            labels={'x': 'ì‘ë‹µë¥ (%)', 'y': 'ì§ˆë¬¸'},
            color=resp.values, color_continuous_scale="viridis"
        ),
        use_container_width=True, key="overview"
    )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2. í†µê³„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
elif page == "ğŸ“ˆ í†µê³„":
    st.markdown('<h2 class="section-header">ğŸ“ˆ ì„ íƒí˜•Â·ì‹œê°„ ë¶„ì„</h2>', unsafe_allow_html=True)
    st.markdown("ğŸ’¡ **í•´ì„¤**: ë¼ë””ì˜¤Â·ì²´í¬ë°•ìŠ¤Â·ì²™ë„Â·ì‹œê°„ ì •ë³´ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.")

    # 2â€‘1) íƒ€ì„ìŠ¤íƒ¬í”„ heatmap
    ts_cols = [c for c, t in configs.items() if t == "timestamp"]
    if ts_cols:
        ts = ts_info(df[ts_cols[0]])
        if ts:
            st.subheader("â° ë‚ ì§œÃ—ì‹œê°„ Heatmap")
            heat_df = ts['heat'].reset_index()
            heat_df['date'] = heat_df['index'].astype(str)
            heat_df[['date', 'hour']] = heat_df['date'].str.split(' ', expand=True)
            heat_df['hour'] = heat_df['hour'].str[:2]
            pivot = heat_df.pivot("date", "hour", "count").fillna(0)
            st.plotly_chart(
                px.imshow(pivot, aspect="auto",
                          labels={'x': 'ì‹œê°„', 'y': 'ë‚ ì§œ', 'color': 'ì‘ë‹µìˆ˜'}),
                use_container_width=True
            )

    # 2â€‘2) ì„ íƒí˜•/ë‹¤ì¤‘ì„ íƒ/ì²™ë„
    for col, t in configs.items():
        if t not in ["single_choice", "multiple_choice", "linear_scale", "numeric"]:
            continue
        st.subheader(f"{col} ({COLUMN_TYPES[t]})")
        series = df[col].dropna().astype(str)

        if t == "single_choice":
            cnt = series.value_counts()
            st.plotly_chart(
                px.pie(cnt, values=cnt.values, names=cnt.index, hole=0.35),
                use_container_width=True
            )
            st.dataframe(cnt.to_frame("ë¹ˆë„"))

        elif t == "multiple_choice":
            # ì‰¼í‘œÂ·ì„¸ë¯¸ì½œë¡  ë¶„ë¦¬
            expanded = series.str.split(r"[;,ï¼|]", expand=True).stack().str.strip()
            cnt = expanded[expanded != ""].value_counts()
            st.plotly_chart(
                px.bar(
                    x=cnt.values, y=cnt.index, orientation="h",
                    labels={'x': 'ë¹ˆë„', 'y': 'ì„ íƒì§€'}
                ),
                use_container_width=True
            )
            st.dataframe(cnt.to_frame("ë¹ˆë„"))

        else:  # linear_scale / numeric
            nums = pd.to_numeric(series, errors="coerce").dropna()
            st.metric("ì‘ë‹µ í‰ê· ", f"{nums.mean():.2f}")
            c1, c2 = st.columns(2)
            with c1:
                st.plotly_chart(
                    px.histogram(nums, nbins=10, labels={'value': 'ê°’'}),
                    use_container_width=True
                )
            with c2:
                st.plotly_chart(
                    px.box(nums, points="all", labels={'value': 'ê°’'}),
                    use_container_width=True
                )
        st.divider()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. í…ìŠ¤íŠ¸ ë¶„ì„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
else:
    st.markdown('<h2 class="section-header">ğŸ“ í…ìŠ¤íŠ¸ ì‘ë‹µ ë¶„ì„</h2>', unsafe_allow_html=True)
    st.markdown("ğŸ’¡ **í•´ì„¤**: ë¹ˆë„ BarÂ·WordCloud, ì¥ë¬¸ ìš”ì•½ì„ í™•ì¸í•  ìˆ˜ ìˆì–´ìš”.")

    for col, t in configs.items():
        if t not in ["text_short", "text_long"]:
            continue
        st.subheader(f"{col} ({'ë‹¨ë‹µ' if t == 'text_short' else 'ì¥ë¬¸'})")
        texts = [str(x) for x in df[col].dropna() if str(x).strip()]
        if len(texts) == 0:
            st.info("ì‘ë‹µ ì—†ìŒ")
            continue

        # â”€ ë¹ˆë„ ë¶„ì„
        tokens = [tok for txt in texts for tok in simple_tokenize(txt)]
        freq = freq_top(tokens)
        if freq:
            words, counts = zip(*freq)
            st.plotly_chart(
                px.bar(x=counts, y=words, orientation="h",
                       labels={'x': 'ë¹ˆë„', 'y': 'ë‹¨ì–´'}),
                use_container_width=True
            )
            st.image(wordcloud_base64(' '.join(tokens)), use_column_width=True)

        # â”€ ì¥ë¬¸ ìš”ì•½
        if t == "text_long" and api_key:
            with st.spinner("AI ìš”ì•½ ìƒì„± ì¤‘..."):
                summary = ai.summarize_large(texts, col)
            st.success("### ğŸ“ 3â€‘ì¤„ ìš”ì•½\n" + summary)

        st.divider()
