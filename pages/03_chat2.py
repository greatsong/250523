"""
AI ì„¤ë¬¸ ëŒ€ì‹œë³´ë“œ ğŸš€ (Streamlit Cloud ì „ìš©)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- í•œê¸€ í°íŠ¸ ìë™ ë‹¤ìš´ë¡œë“œ â†’ WordCloud/Plotly ê¹¨ì§ ë°©ì§€
- ë‹¨ë‹µ/ì¥ë¬¸ ìë™ íŒë³„Â·ë¹ˆë„ ë¶„ì„Â·GPT ìš”ì•½
- ì„ íƒí˜•/ë‹¤ì¤‘ì„ íƒ/ì²™ë„ ì‹œê°í™”
- WordCloud í¬ê¸° ìŠ¬ë¼ì´ë”(ì‚¬ì´ë“œë°”)
- Plotly í…Œë§ˆ plotly_white
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import koreanize_matplotlib
import re, json, textwrap, io, base64, os, random, urllib.request, tempfile, pathlib
from datetime import datetime
from collections import Counter
from typing import Dict, List
from openai import OpenAI
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from matplotlib import font_manager

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 0. í•œê¸€ í°íŠ¸ ì¤€ë¹„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_korean_font() -> str:
    for cand in [
        "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
        "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc",
    ]:
        if os.path.exists(cand):
            return cand
    url = ("https://raw.githubusercontent.com/google/fonts/main/"
           "ofl/nanumgothic/NanumGothic-Regular.ttf")
    tmp = pathlib.Path(tempfile.gettempdir()) / "NanumGothic.ttf"
    if not tmp.exists():
        urllib.request.urlretrieve(url, tmp)
    return str(tmp)

FONT_PATH = get_korean_font()
plt.rcParams["font.family"] = font_manager.FontProperties(fname=FONT_PATH).get_name()

def koreanize_plotly(fig):
    fig.update_layout(font=dict(family="Nanum Gothic, Noto Sans KR, sans-serif"))
    return fig

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. ìƒìˆ˜ Â· ì „ì—­ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
px.defaults.template = "plotly_white"

COLUMN_TYPES = {
    "timestamp": "íƒ€ì„ìŠ¤íƒ¬í”„", "email": "ì´ë©”ì¼", "phone": "ì „í™”ë²ˆí˜¸", "name": "ì´ë¦„",
    "student_id": "í•™ë²ˆ/ì‚¬ë²ˆ", "numeric": "ìˆ«ì",
    "single_choice": "ë‹¨ì¼ ì„ íƒ", "multiple_choice": "ë‹¤ì¤‘ ì„ íƒ",
    "linear_scale": "ì²™ë„",
    "text_short": "ì§§ì€ í…ìŠ¤íŠ¸", "text_long": "ê¸´ í…ìŠ¤íŠ¸",
    "url": "URL", "other": "ê¸°íƒ€"
}
STOP_KO = {
    'ì€','ëŠ”','ì´','ê°€','ì„','ë¥¼','ì˜','ì—','ì™€','ê³¼','ë„','ë¡œ','ìœ¼ë¡œ','ë§Œ','ì—ì„œ','ê¹Œì§€','ë¶€í„°',
    'ë¼ê³ ','í•˜ê³ ','ìˆë‹¤','ìˆëŠ”','ìˆê³ ','í•©ë‹ˆë‹¤','ì…ë‹ˆë‹¤','ëœë‹¤','í•˜ë©°','í•˜ì—¬','í–ˆë‹¤','í•œë‹¤'
}
TOKEN_REGEX = re.compile(r"[ê°€-í£]{2,}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2. ì„¸ì…˜ ìƒíƒœ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
for k, v in [("configs", {}), ("ai", {})]:
    if k not in st.session_state:
        st.session_state[k] = v

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. AI ë¶„ì„ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class AIAnalyzer:
    def __init__(self, key: str):
        self.client = OpenAI(api_key=key) if key else None
        self.model = "gpt-4o"

    def infer_types(self, df: pd.DataFrame) -> Dict[str, str]:
        heur = {}
        for c in df.columns:
            s = str(df[c].dropna().iloc[0]) if not df[c].dropna().empty else ""
            if re.fullmatch(r"\d{4}[./-]", s[:5]): heur[c] = "timestamp"
            elif "@" in s: heur[c] = "email"
            elif s.isdigit() and len(s) <= 6: heur[c] = "student_id"
            elif s.startswith("http"): heur[c] = "url"
            else: heur[c] = "other"
        if not self.client:
            return heur

        prompt = textwrap.dedent(f"""
        í—¤ë”+ìƒ˜í”Œ:
        {df.head(3).to_csv(index=False)}
        ê¸°ì¡´ ì¶”ì •: {json.dumps(heur, ensure_ascii=False)}
        ê°œì„ í•˜ì—¬ JSON ë°˜í™˜.
        íƒ€ì…: {', '.join(COLUMN_TYPES)}
        """)
        try:
            res = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0
            ).choices[0].message.content
            res = re.sub(r"^```json|```$", "", res, flags=re.I).strip()
            g = json.loads(res)
            return {c: (g.get(c, heur[c]) if g.get(c) in COLUMN_TYPES else heur[c])
                    for c in df.columns}
        except Exception:
            return heur

    def summarize(self, texts: List[str], q: str) -> str:
        if not self.client or not texts:
            return "-"
        prompt = textwrap.dedent(f"""
        Q: {q}
        í•œêµ­ì–´ ì‘ë‹µì„ ì½ê³  í•µì‹¬ ì¸ì‚¬ì´íŠ¸ 3ì¤„ ìš”ì•½ bullet ë°˜í™˜:
        {json.dumps(texts, ensure_ascii=False)}
        """)
        try:
            return self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3
            ).choices[0].message.content.strip()
        except Exception:
            return "ìš”ì•½ ì‹¤íŒ¨"

    def summarize_large(self, texts: List[str], q: str) -> str:
        if not texts:
            return "-"
        if len(texts) > 1000:
            texts = random.sample(texts, 1000)
        chunks, buf, char_sum = [], [], 0
        for t in texts:
            buf.append(t)
            char_sum += len(t)
            if char_sum > 2000:
                chunks.append(self.summarize(buf, q))
                buf, char_sum = [], 0
        if buf:
            chunks.append(self.summarize(buf, q))
        return chunks[0] if len(chunks) == 1 else self.summarize(chunks, q)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4. Utils â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def simple_tokenize(text: str):
    return TOKEN_REGEX.findall(text)

def freq_top(tokens, n=20):
    return Counter([t for t in tokens if t not in STOP_KO]).most_common(n)

def wordcloud_base64(text: str, w=600, h=300):
    wc = WordCloud(
        font_path=FONT_PATH, background_color="white",
        width=w, height=h, max_words=100, relative_scaling=0.3
    ).generate(text)
    fig, ax = plt.subplots(figsize=(w / 100, h / 100))
    ax.imshow(wc, interpolation="bilinear")
    ax.axis("off")
    buf = io.BytesIO()
    plt.tight_layout(pad=0)
    fig.savefig(buf, format="png", bbox_inches="tight")
    plt.close(fig)
    return "data:image/png;base64," + base64.b64encode(buf.getvalue()).decode()

def ts_info(series):
    ts = pd.to_datetime(series, errors="coerce").dropna()
    if ts.empty:
        return None
    return {
        "hour": ts.dt.hour.value_counts().sort_index(),
        "day": ts.dt.date.value_counts().sort_index(),
        "heat": ts.dt.to_period('H').value_counts()
    }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5. ì›¹ UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config("AI ì„¤ë¬¸ ëŒ€ì‹œë³´ë“œ", "ğŸ¤–", layout="wide")

with st.sidebar:
    api_key = st.text_input("OpenAI API Key", type="password")
    auto_type = st.checkbox("ğŸ¤– ì»¬ëŸ¼ ìë™ ì¶”ë¡ ", True)
    wc_width  = st.slider("ì›Œë“œí´ë¼ìš°ë“œ í­(px)", 400, 1000, 600, 50)
    wc_height = st.slider("ì›Œë“œí´ë¼ìš°ë“œ ë†’ì´(px)", 200, 600, 300, 50)

file = st.file_uploader("CSV ì—…ë¡œë“œ", type="csv")
if not file:
    st.stop()
df = pd.read_csv(file)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 6. ì»¬ëŸ¼ íƒ€ì… ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ai = AIAnalyzer(api_key) if api_key else None
if auto_type and api_key and not st.session_state.configs:
    st.session_state.configs = ai.infer_types(df)
if not st.session_state.configs:
    st.session_state.configs = {c: "other" for c in df.columns}
configs = st.session_state.configs

for col in df.columns:
    if configs[col] in ["other", "text_short", "text_long"]:
        max_len = df[col].astype(str).str.len().dropna().max()
        if pd.isna(max_len):
            continue
        configs[col] = "text_short" if max_len < 50 else "text_long"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 7. íƒ€ì… ìˆ˜ë™ ìˆ˜ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with st.expander("ì»¬ëŸ¼ íƒ€ì… í™•ì¸/ìˆ˜ì •", False):
    c1, c2 = st.columns(2)
    for i, col in enumerate(df.columns):
        with (c1 if i % 2 == 0 else c2):
            configs[col] = st.selectbox(
                col, list(COLUMN_TYPES.keys()),
                index=list(COLUMN_TYPES.keys()).index(configs[col]),
                format_func=lambda x: COLUMN_TYPES[x],
                key=f"type_{col}"
            )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 8. Navigation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
page = st.radio("ë©”ë‰´", ["ğŸ“Š ê°œìš”", "ğŸ“ˆ í†µê³„", "ğŸ“ í…ìŠ¤íŠ¸ ë¶„ì„"], horizontal=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 9. í˜ì´ì§€ë³„ ë¡œì§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if page == "ğŸ“Š ê°œìš”":
    st.subheader("ğŸ“Š ì „ì²´ ê°œìš”")
    tot, ques = len(df), len(df.columns)
    comp = df.notna().sum().sum() / (tot * ques) * 100
    st.metric("ì‘ë‹µ", tot)
    st.metric("ì§ˆë¬¸", ques)
    st.metric("í‰ê·  ì™„ë£Œìœ¨", f"{comp:.1f}%")
    resp = (df.notna().sum() / tot * 100).sort_values()
    st.plotly_chart(
        koreanize_plotly(px.bar(
            x=resp.values, y=resp.index, orientation="h",
            labels={'x': 'ì‘ë‹µë¥ (%)', 'y': 'ì§ˆë¬¸'},
            color=resp.values, color_continuous_scale="viridis"
        )),
        use_container_width=True
    )

elif page == "ğŸ“ˆ í†µê³„":
    st.subheader("ğŸ“ˆ ì„ íƒí˜•Â·ì‹œê°„ ë¶„ì„")
    ts_cols = [c for c, t in configs.items() if t == "timestamp"]
    if ts_cols:
        ts = ts_info(df[ts_cols[0]])
        if ts:
            heat_df = ts['heat'].reset_index()
            heat_df['date'] = heat_df['index'].astype(str)
            heat_df[['date', 'hour']] = heat_df['date'].str.split(' ', expand=True)
            heat_df['hour'] = heat_df['hour'].str[:2]
            pivot = heat_df.pivot("date", "hour", "count").fillna(0)
            st.plotly_chart(
                koreanize_plotly(px.imshow(
                    pivot, aspect="auto",
                    labels={'x': 'ì‹œê°„', 'y': 'ë‚ ì§œ', 'color': 'ì‘ë‹µìˆ˜'}
                )),
                use_container_width=True
            )
    for col, t in configs.items():
        if t not in ["single_choice", "multiple_choice", "linear_scale", "numeric"]:
            continue
        st.markdown(f"#### {col} ({COLUMN_TYPES[t]})")
        series = df[col].dropna().astype(str)

        if t == "single_choice":
            cnt = series.value_counts()
            st.plotly_chart(
                koreanize_plotly(px.pie(cnt, values=cnt.values, names=cnt.index, hole=0.35)),
                use_container_width=True
            )
            st.dataframe(cnt.to_frame("ë¹ˆë„"))

        elif t == "multiple_choice":
            expanded = series.str.split(r"[;,ï¼|]", expand=True).stack().str.strip()
            cnt = expanded[expanded != ""].value_counts()
            st.plotly_chart(
                koreanize_plotly(px.bar(
                    x=cnt.values, y=cnt.index, orientation="h",
                    labels={'x': 'ë¹ˆë„', 'y': 'ì„ íƒì§€'}
                )),
                use_container_width=True
            )
            st.dataframe(cnt.to_frame("ë¹ˆë„"))

        else:
            nums = pd.to_numeric(series, errors="coerce").dropna()
            st.metric("ì‘ë‹µ í‰ê· ", f"{nums.mean():.2f}")
            c1, c2 = st.columns(2)
            with c1:
                st.plotly_chart(
                    koreanize_plotly(px.histogram(nums, nbins=10, labels={'value': 'ê°’'})),
                    use_container_width=True
                )
            with c2:
                st.plotly_chart(
                    koreanize_plotly(px.box(nums, points="all", labels={'value': 'ê°’'})),
                    use_container_width=True
                )
        st.divider()

else:
    st.subheader("ğŸ“ í…ìŠ¤íŠ¸ ì‘ë‹µ ë¶„ì„")
    for col, t in configs.items():
        if t not in ["text_short", "text_long"]:
            continue
        st.markdown(f"#### {col} ({'ë‹¨ë‹µ' if t == 'text_short' else 'ì¥ë¬¸'})")
        texts = [str(x) for x in df[col].dropna() if str(x).strip()]
        if not texts:
            st.info("ì‘ë‹µ ì—†ìŒ")
            continue

        tokens = [tok for txt in texts for tok in simple_tokenize(txt)]
        freq = freq_top(tokens)
        if freq:
            words, counts = zip(*freq)
            st.plotly_chart(
                koreanize_plotly(px.bar(
                    x=counts, y=words, orientation="h",
                    labels={'x': 'ë¹ˆë„', 'y': 'ë‹¨ì–´'}
                )),
                use_container_width=True
            )
            st.image(
                wordcloud_base64(' '.join(tokens), wc_width, wc_height),
                use_container_width=True
            )

        if t == "text_long" and api_key:
            st.caption("ğŸ§  GPT ìš”ì•½")
            with st.spinner("ìš”ì•½ ì¤‘..."):
                summary = ai.summarize_large(texts, col)
            st.success(summary)
        st.divider()
