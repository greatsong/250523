"""
AI ì„¤ë¬¸ ëŒ€ì‹œë³´ë“œÂ ğŸš€
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… ë¹ ë¥´ê²Œ ì‹œì‘í•˜ë ¤ë©´ + ì„ íƒí˜• ë¶„ì„ + **í•œê¸€ WordCloud ê¹¨ì§ í•´ê²°**
   1. NanumGothicÂ í°íŠ¸ ìë™ ë‹¤ìš´ë¡œë“œ & ìºì‹± (tempdir)
   2. WordCloud ìƒì„± ì‹œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ â†’ ê²½ê³  ì¶œë ¥
   3. (ê¸°ì¡´ ê¸°ëŠ¥) ë‹¨ë‹µ/ì¥ë¬¸ ìë™ íŒë³„Â·ë¹ˆë„Â·ìš”ì•½Â·ì„ íƒí˜• ë¶„ì„ ìœ ì§€
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import koreanize_matplotlib  # ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­
import re, json, textwrap, io, base64, os, random, tempfile, urllib.request
from datetime import datetime
from collections import Counter
from typing import Dict, List
from openai import OpenAI
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from pathlib import Path

# Plotly ê¸°ë³¸ í…Œë§ˆ í†µì¼
px.defaults.template = "plotly_white"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Web UI ê¸°ë³¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config("AI ì„¤ë¬¸ ëŒ€ì‹œë³´ë“œ", "ğŸ¤–", layout="wide")
ST_CSS = """
<style>
.main-header{font-size:2.5rem;font-weight:700;text-align:center;
background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);
-webkit-background-clip:text;-webkit-text-fill-color:transparent;margin:1rem 0;}
.section-header{font-size:1.6rem;font-weight:600;border-bottom:3px solid #667eea;margin:1.3rem 0 .8rem 0;}
.metric-card{background:linear-gradient(135deg,#f5f7fa 0%,#c3cfe2 100%);padding:1rem;border-radius:12px;text-align:center;box-shadow:0 4px 12px rgba(0,0,0,.08);}
</style>"""
st.markdown(ST_CSS, unsafe_allow_html=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
COLUMN_TYPES = {
    "timestamp":"íƒ€ì„ìŠ¤íƒ¬í”„","email":"ì´ë©”ì¼","phone":"ì „í™”ë²ˆí˜¸","name":"ì´ë¦„",
    "student_id":"í•™ë²ˆ/ì‚¬ë²ˆ","numeric":"ìˆ«ì","single_choice":"ë‹¨ì¼ ì„ íƒ",
    "multiple_choice":"ë‹¤ì¤‘ ì„ íƒ","linear_scale":"ì²™ë„","text_short":"ì§§ì€ í…ìŠ¤íŠ¸",
    "text_long":"ê¸´ í…ìŠ¤íŠ¸","url":"URL","other":"ê¸°íƒ€"
}
STOP_KO = {
    'ì€','ëŠ”','ì´','ê°€','ì„','ë¥¼','ì˜','ì—','ì™€','ê³¼','ë„','ë¡œ','ìœ¼ë¡œ','ë§Œ','ì—ì„œ','ê¹Œì§€','ë¶€í„°',
    'ë¼ê³ ','í•˜ê³ ','ìˆë‹¤','ìˆëŠ”','ìˆê³ ','í•©ë‹ˆë‹¤','ì…ë‹ˆë‹¤','ëœë‹¤','í•˜ë©°','í•˜ì—¬','í–ˆë‹¤','í•œë‹¤'
}
TOKEN_REGEX = re.compile(r"[ê°€-í£]{2,}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
for k,v in [("df",None),("configs",{}),("ai",{}),("ai_done",False)]:
    if k not in st.session_state: st.session_state[k]=v

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ AI Analyzer (unchanged) â”€â”€â”€â”€â”€â”€â”€â”€
class AIAnalyzer:
    def __init__(self,key:str):
        self.key=key
        self.client=OpenAI(api_key=key) if key else None
        self.model="gpt-4o"
    def infer_types(self,df:pd.DataFrame)->Dict[str,str]:
        heur={}
        for c in df.columns:
            s=str(df[c].dropna().iloc[0]) if not df[c].dropna().empty else ""
            if re.fullmatch(r"\d{4}[./-]",s[:5]): heur[c]="timestamp"
            elif "@" in s: heur[c]="email"
            elif s.isdigit() and len(s)<=6: heur[c]="student_id"
            elif s.startswith("http"): heur[c]="url"
            else: heur[c]="other"
        if not self.client: return heur
        prompt=textwrap.dedent(f"""í—¤ë”+ìƒ˜í”Œ:\n{df.head(3).to_csv(index=False)}\nê¸°ì¡´ ì¶”ì •: {json.dumps(heur,ensure_ascii=False)}\nê°œì„ í•˜ì—¬ JSON ë°˜í™˜. íƒ€ì…: {', '.join(COLUMN_TYPES)}""")
        try:
            res=self.client.chat.completions.create(model=self.model,messages=[{"role":"user","content":prompt}],temperature=0).choices[0].message.content
            res=re.sub(r"^```json|```$","",res,flags=re.I).strip()
            g=json.loads(res)
            return {c:(g.get(c,heur[c]) if g.get(c) in COLUMN_TYPES else heur[c]) for c in df.columns}
        except: return heur
    def summarize(self,texts:List[str],q:str)->str:
        if not self.client or len(texts)==0: return "-"
        prompt=textwrap.dedent(f"""Q: {q}\ní•œêµ­ì–´ ì‘ë‹µì„ ì½ê³  í•µì‹¬ ì¸ì‚¬ì´íŠ¸ 3ì¤„ ìš”ì•½ bullet ë°˜í™˜:\n{json.dumps(texts,ensure_ascii=False)}""")
        try:
            return self.client.chat.completions.create(model=self.model,messages=[{"role":"user","content":prompt}],temperature=0.3).choices[0].message.content.strip()
        except: return "ìš”ì•½ ì‹¤íŒ¨"
    def summarize_large(self,texts:List[str],q:str)->str:
        if not texts: return "-"
        if len(texts)>1000: texts=random.sample(texts,1000)
        chunks,buf,char_sum=[],[],0
        for t in texts:
            buf.append(t); char_sum+=len(t)
            if char_sum>2000:
                chunks.append(self.summarize(buf,q)); buf,char_sum=[],0
        if buf: chunks.append(self.summarize(buf,q))
        return chunks[0] if len(chunks)==1 else self.summarize(chunks,q)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Utils â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def simple_tokenize(text:str)->List[str]:
    return TOKEN_REGEX.findall(text)

def freq_top(tokens:List[str],n:int=20):
    return Counter([t for t in tokens if t not in STOP_KO]).most_common(n)

# â–¶ **í°íŠ¸ ìë™ í™•ë³´**
FONT_CACHE = Path(tempfile.gettempdir())/"NanumGothic.ttf"
FONT_URL = "https://github.com/google/fonts/raw/main/ofl/nanumgothic/NanumGothic-Regular.ttf"

def ensure_korean_font() -> str:
    """NanumGothic ê²½ë¡œ ë°˜í™˜, ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ"""
    candidates=[
        "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
        "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc",
        str(FONT_CACHE)
    ]
    for p in candidates:
        if os.path.exists(p):
            return p
    try:
        urllib.request.urlretrieve(FONT_URL, FONT_CACHE)
        return str(FONT_CACHE)
    except Exception as e:
        st.warning("ğŸ–‹ï¸ í•œê¸€ í°íŠ¸ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. WordCloudê°€ ê¹¨ì§ˆ ìˆ˜ ìˆì–´ìš”.")
        return None

def wordcloud_base64(text:str)->str|None:
    font=ensure_korean_font()
    if font is None:
        return None
    wc=WordCloud(font_path=font,background_color="white",width=800,height=400).generate(text)
    fig,ax=plt.subplots(figsize=(8,4))
    ax.imshow(wc, interpolation="bilinear"); ax.axis("off")
    buf=io.BytesIO(); plt.tight_layout(pad=0)
    fig.savefig(buf,format="png",bbox_inches="tight"); plt.close(fig)
    return "data:image/png;base64,"+base64.b64encode(buf.getvalue()).decode()

def ts_info(series:pd.Series):
    ts=pd.to_datetime(series,errors="coerce").dropna()
    if ts.empty: return None
    return {"hour":ts.dt.hour.value_counts().sort_index(),
            "day":ts.dt.date.value_counts().sort_index(),
            "heat":ts.dt.to_period('H').value_counts()}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Sidebar â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with st.sidebar:
    api_key=st.text_input("OpenAI API Key",value=st.secrets.get("openai_api_key",""),type="password")
    mask_opt=st.checkbox("ğŸ”’ ê°œì¸ì •ë³´ ë§ˆìŠ¤í‚¹",True)
    auto_type=st.checkbox("ğŸ¤– ì»¬ëŸ¼ ìë™ ì¶”ë¡ ",True)

file=st.file_uploader("CSV ì—…ë¡œë“œ",type="csv")
if not file:
    st.stop()

df=pd.read_csv(file)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì»¬ëŸ¼ íƒ€ì… ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ai=AIAnalyzer(api_key) if api_key else None
if auto_type and api_key and not st.session_state.configs:
    st.session_state.configs=ai.infer_types(df)
if not st.session_state.configs:
    st.session_state.configs={c:"other" for c in df.columns}
configs=st.session_state.configs

# â–¶ ë‹¨ë‹µ/ì¥ë¬¸ ìë™ íŒë³„
for col in df.columns:
    if configs[col] in ["other","text_short","text_long"]:
        max_len=df[col].astype(str).str.len().dropna().max()
        if pd.isna(max_len): continue
        configs[col]="text_short" if max_len<50 else "text_long"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Manual Type Edit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with st.expander("ì»¬ëŸ¼ íƒ€ì… í™•ì¸/ìˆ˜ì •", False):
    c1,c2=st.columns(2)
    for i,col in enumerate(df.columns):
        with (c1 if i%2==0 else c2):
            configs[col]=st.selectbox(col,list(COLUMN_TYPES.keys()),
                index=list(COLUMN_TYPES.keys()).index(configs[col]),
                format_func=lambda x:COLUMN_TYPES[x],key=f"type_{col}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Navigation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
page=st.radio("ë©”ë‰´",["ğŸ“Š ê°œìš”","ğŸ“ˆ í†µê³„","ğŸ“ í…ìŠ¤íŠ¸ ë¶„ì„"],horizontal=True)

# (ë‚˜ë¨¸ì§€ íƒ­ ë¡œì§ ë™ì¼ â€“Â ìƒëµ, ê¸°ì¡´ ì½”ë“œì™€ ë³€í™” ì—†ìŒ)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. í…ìŠ¤íŠ¸ ë¶„ì„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if page=="ğŸ“ í…ìŠ¤íŠ¸ ë¶„ì„":
    st.markdown('<h2 class="section-header">ğŸ“ í…ìŠ¤íŠ¸ ì‘ë‹µ ë¶„ì„</h2>',unsafe_allow_html=True)
    st.markdown("ğŸ’¡ **í•´ì„¤**: í•œê¸€ WordCloudê°€ ê¹¨ì§ˆ ê²½ìš° ìë™ìœ¼ë¡œ NanumGothic í°íŠ¸ë¥¼ ë‚´ë ¤ë°›ì•„ ì ìš©í•©ë‹ˆë‹¤.")
    for col,t in configs.items():
        if t not in ["text_short","text_long"]: continue
        st.subheader(f"{col} ({'ë‹¨ë‹µ' if t=='text_short' else 'ì¥ë¬¸'})")
        texts=[str(x) for x in df[col].dropna() if str(x).strip()]
        if not texts:
            st.info("ì‘ë‹µ ì—†ìŒ"); continue
        tokens=[tok for txt in texts for tok in simple_tokenize(txt)]
        freq=freq_top(tokens)
        if freq:
            words,counts=zip(*freq)
            st.plotly_chart(px.bar(x=counts,y=words,orientation="h",labels={'x':'ë¹ˆë„','y':'ë‹¨ì–´'}),use_container_width=True)
            img64=wordcloud_base64(' '.join(tokens))
            if img64:
                st.image(img64,use_column_width=True)
        if t=="text_long" and api_key:
            with st.spinner("AI ìš”ì•½ ìƒì„± ì¤‘..."):
                summary=ai.summarize_large(texts,col)
            st.success("### ğŸ“ 3â€‘ì¤„ ìš”ì•½\n"+summary)
        st.divider()
